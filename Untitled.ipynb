{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(r'train_set.csv', sep='\\t', encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'category')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "train_df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('News class count')\n",
    "plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3750', 197997)\n",
      "('900', 197653)\n",
      "('648', 191975)\n"
     ]
    }
   ],
   "source": [
    "train_df['text_unique'] = train_df['text'].apply(\n",
    "    lambda x: ' '.join(list(set(x.split(' ')))))\n",
    "all_lines = ' '.join(list(train_df['text_unique']))\n",
    "word_count = Counter(all_lines.split(\" \"))\n",
    "word_count = sorted(word_count.items(), key=lambda d: int(d[1]), reverse=True)\n",
    "print(word_count[0])\n",
    "print(word_count[1])\n",
    "print(word_count[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416952793751392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "train_df = pd.read_csv('train_set.csv', sep='\\t', nrows=15000)\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(train_df['text'])\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8721598830546126\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.metrics import f1_score\n",
    "# train_df = pd.read_csv('../input/train_set.csv', sep='\\t', nrows=15000)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastTxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                1414      \n",
      "=================================================================\n",
      "Total params: 5,001,414\n",
      "Trainable params: 5,001,414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# %reset \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS =500\n",
    "CLASS_NUM = 14\n",
    "\n",
    "def build_fastText():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAX_WORDS))\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(CLASS_NUM,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "    return model\n",
    "model = build_fastText()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train_set.csv', sep='\\t', nrows=50000)\n",
    "train_df['text'] = train_df['text'].apply(lambda x:x[0:2500])\n",
    "train_df_sp_dim = train_df.text.str.split(' ',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sp_dim.iloc[:,0:500].join(train_df.label).to_csv(\"train_df_sp_dim.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df_sp_dim.iloc[:, 0:500], train_df[['label']], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26250 samples, validate on 8750 samples\n",
      "Epoch 1/50\n",
      "26250/26250 [==============================] - 5s 201us/sample - loss: 0.2359 - accuracy: 0.9357 - val_loss: 0.3646 - val_accuracy: 0.8951\n",
      "Epoch 2/50\n",
      "26250/26250 [==============================] - 5s 200us/sample - loss: 0.2318 - accuracy: 0.9367 - val_loss: 0.3634 - val_accuracy: 0.8960\n",
      "Epoch 3/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.2280 - accuracy: 0.9377 - val_loss: 0.3627 - val_accuracy: 0.8959\n",
      "Epoch 4/50\n",
      "26250/26250 [==============================] - 5s 193us/sample - loss: 0.2241 - accuracy: 0.9384 - val_loss: 0.3617 - val_accuracy: 0.8973\n",
      "Epoch 5/50\n",
      "26250/26250 [==============================] - 5s 198us/sample - loss: 0.2204 - accuracy: 0.9402 - val_loss: 0.3607 - val_accuracy: 0.8973\n",
      "Epoch 6/50\n",
      "26250/26250 [==============================] - 5s 200us/sample - loss: 0.2167 - accuracy: 0.9416 - val_loss: 0.3601 - val_accuracy: 0.8976\n",
      "Epoch 7/50\n",
      "26250/26250 [==============================] - 5s 205us/sample - loss: 0.2132 - accuracy: 0.9426 - val_loss: 0.3593 - val_accuracy: 0.8973\n",
      "Epoch 8/50\n",
      "26250/26250 [==============================] - 5s 206us/sample - loss: 0.2099 - accuracy: 0.9435 - val_loss: 0.3589 - val_accuracy: 0.8979\n",
      "Epoch 9/50\n",
      "26250/26250 [==============================] - 5s 203us/sample - loss: 0.2065 - accuracy: 0.9438 - val_loss: 0.3584 - val_accuracy: 0.8981\n",
      "Epoch 10/50\n",
      "26250/26250 [==============================] - 5s 203us/sample - loss: 0.2031 - accuracy: 0.9451 - val_loss: 0.3579 - val_accuracy: 0.8976\n",
      "Epoch 11/50\n",
      "26250/26250 [==============================] - 5s 199us/sample - loss: 0.1999 - accuracy: 0.9462 - val_loss: 0.3573 - val_accuracy: 0.8984\n",
      "Epoch 12/50\n",
      "26250/26250 [==============================] - 5s 198us/sample - loss: 0.1966 - accuracy: 0.9468 - val_loss: 0.3573 - val_accuracy: 0.8991\n",
      "Epoch 13/50\n",
      "26250/26250 [==============================] - 5s 199us/sample - loss: 0.1934 - accuracy: 0.9481 - val_loss: 0.3571 - val_accuracy: 0.8984\n",
      "Epoch 14/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1904 - accuracy: 0.9488 - val_loss: 0.3568 - val_accuracy: 0.8989\n",
      "Epoch 15/50\n",
      "26250/26250 [==============================] - 5s 193us/sample - loss: 0.1873 - accuracy: 0.9497 - val_loss: 0.3570 - val_accuracy: 0.8984\n",
      "Epoch 16/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1845 - accuracy: 0.9504 - val_loss: 0.3564 - val_accuracy: 0.8979\n",
      "Epoch 17/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1816 - accuracy: 0.9509 - val_loss: 0.3565 - val_accuracy: 0.8994\n",
      "Epoch 18/50\n",
      "26250/26250 [==============================] - 5s 193us/sample - loss: 0.1786 - accuracy: 0.9517 - val_loss: 0.3564 - val_accuracy: 0.8993\n",
      "Epoch 19/50\n",
      "26250/26250 [==============================] - 5s 193us/sample - loss: 0.1758 - accuracy: 0.9525 - val_loss: 0.3572 - val_accuracy: 0.8989\n",
      "Epoch 20/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1733 - accuracy: 0.9535 - val_loss: 0.3568 - val_accuracy: 0.8983\n",
      "Epoch 21/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1705 - accuracy: 0.9553 - val_loss: 0.3573 - val_accuracy: 0.8983\n",
      "Epoch 22/50\n",
      "26250/26250 [==============================] - 5s 198us/sample - loss: 0.1678 - accuracy: 0.9554 - val_loss: 0.3571 - val_accuracy: 0.8989\n",
      "Epoch 23/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1651 - accuracy: 0.9561 - val_loss: 0.3573 - val_accuracy: 0.8986\n",
      "Epoch 24/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1625 - accuracy: 0.9565 - val_loss: 0.3580 - val_accuracy: 0.8971\n",
      "Epoch 25/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1600 - accuracy: 0.9571 - val_loss: 0.3580 - val_accuracy: 0.8989\n",
      "Epoch 26/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1575 - accuracy: 0.9583 - val_loss: 0.3587 - val_accuracy: 0.8987\n",
      "Epoch 27/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1549 - accuracy: 0.9592 - val_loss: 0.3587 - val_accuracy: 0.8985\n",
      "Epoch 28/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1526 - accuracy: 0.9595 - val_loss: 0.3596 - val_accuracy: 0.8984\n",
      "Epoch 29/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1502 - accuracy: 0.9606 - val_loss: 0.3598 - val_accuracy: 0.8982\n",
      "Epoch 30/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1477 - accuracy: 0.9617 - val_loss: 0.3608 - val_accuracy: 0.8981\n",
      "Epoch 31/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1454 - accuracy: 0.9622 - val_loss: 0.3610 - val_accuracy: 0.8992\n",
      "Epoch 32/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1432 - accuracy: 0.9629 - val_loss: 0.3620 - val_accuracy: 0.8985\n",
      "Epoch 33/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1410 - accuracy: 0.9638 - val_loss: 0.3627 - val_accuracy: 0.8977\n",
      "Epoch 34/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1388 - accuracy: 0.9645 - val_loss: 0.3633 - val_accuracy: 0.8987\n",
      "Epoch 35/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1366 - accuracy: 0.9654 - val_loss: 0.3646 - val_accuracy: 0.8983\n",
      "Epoch 36/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1345 - accuracy: 0.9655 - val_loss: 0.3647 - val_accuracy: 0.8990\n",
      "Epoch 37/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1324 - accuracy: 0.9661 - val_loss: 0.3656 - val_accuracy: 0.8985\n",
      "Epoch 38/50\n",
      "26250/26250 [==============================] - 5s 194us/sample - loss: 0.1301 - accuracy: 0.9672 - val_loss: 0.3666 - val_accuracy: 0.8994\n",
      "Epoch 39/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1282 - accuracy: 0.9675 - val_loss: 0.3675 - val_accuracy: 0.8982\n",
      "Epoch 40/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1262 - accuracy: 0.9680 - val_loss: 0.3682 - val_accuracy: 0.8979\n",
      "Epoch 41/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1241 - accuracy: 0.9690 - val_loss: 0.3695 - val_accuracy: 0.8978\n",
      "Epoch 42/50\n",
      "26250/26250 [==============================] - 5s 197us/sample - loss: 0.1224 - accuracy: 0.9693 - val_loss: 0.3706 - val_accuracy: 0.8978\n",
      "Epoch 43/50\n",
      "26250/26250 [==============================] - 5s 195us/sample - loss: 0.1204 - accuracy: 0.9699 - val_loss: 0.3709 - val_accuracy: 0.8990\n",
      "Epoch 44/50\n",
      "26250/26250 [==============================] - 5s 196us/sample - loss: 0.1185 - accuracy: 0.9710 - val_loss: 0.3723 - val_accuracy: 0.8971\n",
      "Epoch 45/50\n",
      "26250/26250 [==============================] - 5s 201us/sample - loss: 0.1165 - accuracy: 0.9710 - val_loss: 0.3735 - val_accuracy: 0.8974\n",
      "Epoch 46/50\n",
      "26250/26250 [==============================] - 5s 202us/sample - loss: 0.1148 - accuracy: 0.9715 - val_loss: 0.3744 - val_accuracy: 0.8973\n",
      "Epoch 47/50\n",
      "26250/26250 [==============================] - 5s 201us/sample - loss: 0.1128 - accuracy: 0.9720 - val_loss: 0.3757 - val_accuracy: 0.8975\n",
      "Epoch 48/50\n",
      "26250/26250 [==============================] - 5s 202us/sample - loss: 0.1111 - accuracy: 0.9730 - val_loss: 0.3765 - val_accuracy: 0.8975\n",
      "Epoch 49/50\n",
      "26250/26250 [==============================] - 5s 199us/sample - loss: 0.1096 - accuracy: 0.9734 - val_loss: 0.3778 - val_accuracy: 0.8973\n",
      "Epoch 50/50\n",
      "26250/26250 [==============================] - 5s 200us/sample - loss: 0.1076 - accuracy: 0.9738 - val_loss: 0.3791 - val_accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c859888>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_df.label_ft.unique())\n",
    "# train_df.label_ft.unique()\n",
    "# model = build_fastText()\n",
    "model.fit(X_train.astype('float64'), y_train.astype('float64'), batch_size=512,\n",
    "          epochs=50, validation_split=0.25,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "def textcnn( max_sequence_length, max_token_num, embedding_dim, output_dim, model_img_path=None, embedding_matrix=None):\n",
    "    \"\"\" TextCNN: 1. embedding layers, 2.convolution layer, 3.max-pooling, 4.softmax layer. \"\"\"\n",
    "    x_input = Input(shape=(max_sequence_length,))\n",
    "    logging.info(\"x_input.shape: %s\" % str(x_input.shape))  # (?, 60)\n",
    "\n",
    "    if embedding_matrix is None:\n",
    "        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim,\n",
    "                          input_length=max_sequence_length)(x_input)\n",
    "    else:\n",
    "        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim, input_length=max_sequence_length,\n",
    "                          weights=[embedding_matrix], trainable=True)(x_input)\n",
    "    logging.info(\"x_emb.shape: %s\" % str(x_emb.shape))  # (?, 60, 300)\n",
    "\n",
    "    pool_output = []\n",
    "    kernel_sizes = [1,2, 3, 4]\n",
    "    for kernel_size in kernel_sizes:\n",
    "        c = Conv1D(filters=1, kernel_size=kernel_size, strides=1)(x_emb)\n",
    "        p = MaxPool1D(pool_size=int(c.shape[1]))(c)\n",
    "        pool_output.append(p)\n",
    "        logging.info(\"kernel_size: %s \\t c.shape: %s \\t p.shape: %s\" %\n",
    "                     (kernel_size, str(c.shape), str(p.shape)))\n",
    "    pool_output = concatenate([p for p in pool_output])\n",
    "    logging.info(\"pool_output.shape: %s\" % str(pool_output.shape))  # (?, 1, 6)\n",
    "\n",
    "    x_flatten = Flatten()(pool_output)  # (?, 6)\n",
    "    y = Dense(output_dim, activation='softmax')(x_flatten)  # (?, 2)\n",
    "    logging.info(\"y.shape: %s \\n\" % str(y.shape))\n",
    "\n",
    "    model = Model([x_input], outputs=[y])\n",
    "    if model_img_path:\n",
    "        plot_model(model, to_file=model_img_path,\n",
    "                   show_shapes=True, show_layer_names=False)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train_set.csv', sep='\\t', nrows=130000)\n",
    "train_df['text'] = train_df['text'].apply(lambda x:x[0:2500])\n",
    "train_df_sp_dim = train_df.text.str.split(' ',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df_sp_dim.iloc[:, 0:500].astype('float64'), train_df[['label']].astype('float64'), test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 100)     5000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 500, 1)       101         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 499, 1)       201         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 498, 1)       301         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 497, 1)       401         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 1)         0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 1)         0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 1)         0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1, 1)         0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 4)         0           max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4)            0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 14)           70          flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,001,074\n",
      "Trainable params: 5,001,074\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS = 500\n",
    "CLASS_NUM = 14\n",
    "model = textcnn( max_sequence_length=MAX_WORDS,\n",
    "        max_token_num=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, output_dim=CLASS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68250 samples, validate on 22750 samples\n",
      "Epoch 1/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 1.9313 - accuracy: 0.3836 - val_loss: 1.4773 - val_accuracy: 0.5616\n",
      "Epoch 2/50\n",
      "68250/68250 [==============================] - 28s 416us/sample - loss: 1.2698 - accuracy: 0.6248 - val_loss: 1.1414 - val_accuracy: 0.6593\n",
      "Epoch 3/50\n",
      "68250/68250 [==============================] - 28s 415us/sample - loss: 1.0435 - accuracy: 0.6842 - val_loss: 1.0251 - val_accuracy: 0.6923\n",
      "Epoch 4/50\n",
      "68250/68250 [==============================] - 28s 417us/sample - loss: 0.9352 - accuracy: 0.7148 - val_loss: 0.9707 - val_accuracy: 0.7118\n",
      "Epoch 5/50\n",
      "68250/68250 [==============================] - 29s 419us/sample - loss: 0.8713 - accuracy: 0.7350 - val_loss: 0.9372 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "68250/68250 [==============================] - 29s 418us/sample - loss: 0.8272 - accuracy: 0.7474 - val_loss: 0.9155 - val_accuracy: 0.7280\n",
      "Epoch 7/50\n",
      "68250/68250 [==============================] - 29s 421us/sample - loss: 0.7922 - accuracy: 0.7595 - val_loss: 0.9039 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "68250/68250 [==============================] - 29s 418us/sample - loss: 0.7643 - accuracy: 0.7669 - val_loss: 0.8933 - val_accuracy: 0.7372\n",
      "Epoch 9/50\n",
      "68250/68250 [==============================] - 29s 420us/sample - loss: 0.7381 - accuracy: 0.7764 - val_loss: 0.8853 - val_accuracy: 0.7439\n",
      "Epoch 10/50\n",
      "68250/68250 [==============================] - 29s 421us/sample - loss: 0.7161 - accuracy: 0.7831 - val_loss: 0.8809 - val_accuracy: 0.7472\n",
      "Epoch 11/50\n",
      "68250/68250 [==============================] - 29s 421us/sample - loss: 0.6991 - accuracy: 0.7886 - val_loss: 0.8786 - val_accuracy: 0.7486\n",
      "Epoch 12/50\n",
      "68250/68250 [==============================] - 29s 421us/sample - loss: 0.6849 - accuracy: 0.7928 - val_loss: 0.8717 - val_accuracy: 0.7512\n",
      "Epoch 13/50\n",
      "68250/68250 [==============================] - 29s 420us/sample - loss: 0.6711 - accuracy: 0.7982 - val_loss: 0.8715 - val_accuracy: 0.7544\n",
      "Epoch 14/50\n",
      "68250/68250 [==============================] - 29s 420us/sample - loss: 0.6590 - accuracy: 0.8026 - val_loss: 0.8655 - val_accuracy: 0.7555\n",
      "Epoch 15/50\n",
      "68250/68250 [==============================] - 29s 424us/sample - loss: 0.6454 - accuracy: 0.8075 - val_loss: 0.8581 - val_accuracy: 0.7611\n",
      "Epoch 16/50\n",
      "68250/68250 [==============================] - 29s 420us/sample - loss: 0.6337 - accuracy: 0.8122 - val_loss: 0.8580 - val_accuracy: 0.7622\n",
      "Epoch 17/50\n",
      "68250/68250 [==============================] - 29s 429us/sample - loss: 0.6238 - accuracy: 0.8168 - val_loss: 0.8598 - val_accuracy: 0.7636\n",
      "Epoch 18/50\n",
      "68250/68250 [==============================] - 29s 421us/sample - loss: 0.6138 - accuracy: 0.8211 - val_loss: 0.8568 - val_accuracy: 0.7661\n",
      "Epoch 19/50\n",
      "68250/68250 [==============================] - 31s 453us/sample - loss: 0.6065 - accuracy: 0.8235 - val_loss: 0.8605 - val_accuracy: 0.7655\n",
      "Epoch 20/50\n",
      "68250/68250 [==============================] - 34s 503us/sample - loss: 0.5992 - accuracy: 0.8258 - val_loss: 0.8598 - val_accuracy: 0.7662\n",
      "Epoch 21/50\n",
      "68250/68250 [==============================] - 31s 454us/sample - loss: 0.5938 - accuracy: 0.8279 - val_loss: 0.8610 - val_accuracy: 0.7670\n",
      "Epoch 22/50\n",
      "68250/68250 [==============================] - 30s 439us/sample - loss: 0.5878 - accuracy: 0.8296 - val_loss: 0.8667 - val_accuracy: 0.7654\n",
      "Epoch 23/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5823 - accuracy: 0.8314 - val_loss: 0.8688 - val_accuracy: 0.7671\n",
      "Epoch 24/50\n",
      "68250/68250 [==============================] - 30s 436us/sample - loss: 0.5785 - accuracy: 0.8331 - val_loss: 0.8613 - val_accuracy: 0.7699\n",
      "Epoch 25/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5738 - accuracy: 0.8345 - val_loss: 0.8673 - val_accuracy: 0.7713\n",
      "Epoch 26/50\n",
      "68250/68250 [==============================] - 30s 438us/sample - loss: 0.5696 - accuracy: 0.8364 - val_loss: 0.8651 - val_accuracy: 0.7723\n",
      "Epoch 27/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5655 - accuracy: 0.8388 - val_loss: 0.8673 - val_accuracy: 0.7718\n",
      "Epoch 28/50\n",
      "68250/68250 [==============================] - 30s 440us/sample - loss: 0.5618 - accuracy: 0.8395 - val_loss: 0.8698 - val_accuracy: 0.7709\n",
      "Epoch 29/50\n",
      "68250/68250 [==============================] - 30s 438us/sample - loss: 0.5588 - accuracy: 0.8406 - val_loss: 0.8705 - val_accuracy: 0.7716\n",
      "Epoch 30/50\n",
      "68250/68250 [==============================] - 30s 442us/sample - loss: 0.5549 - accuracy: 0.8410 - val_loss: 0.8738 - val_accuracy: 0.7731\n",
      "Epoch 31/50\n",
      "68250/68250 [==============================] - 30s 438us/sample - loss: 0.5524 - accuracy: 0.8419 - val_loss: 0.8717 - val_accuracy: 0.7736\n",
      "Epoch 32/50\n",
      "68250/68250 [==============================] - 30s 440us/sample - loss: 0.5498 - accuracy: 0.8438 - val_loss: 0.8743 - val_accuracy: 0.7720\n",
      "Epoch 33/50\n",
      "68250/68250 [==============================] - 30s 442us/sample - loss: 0.5470 - accuracy: 0.8437 - val_loss: 0.8785 - val_accuracy: 0.7724\n",
      "Epoch 34/50\n",
      "68250/68250 [==============================] - 30s 446us/sample - loss: 0.5448 - accuracy: 0.8441 - val_loss: 0.8783 - val_accuracy: 0.7731\n",
      "Epoch 35/50\n",
      "68250/68250 [==============================] - 30s 445us/sample - loss: 0.5420 - accuracy: 0.8449 - val_loss: 0.8782 - val_accuracy: 0.7720\n",
      "Epoch 36/50\n",
      "68250/68250 [==============================] - 30s 444us/sample - loss: 0.5405 - accuracy: 0.8450 - val_loss: 0.8819 - val_accuracy: 0.7719\n",
      "Epoch 37/50\n",
      "68250/68250 [==============================] - 30s 440us/sample - loss: 0.5383 - accuracy: 0.8467 - val_loss: 0.8822 - val_accuracy: 0.7738\n",
      "Epoch 38/50\n",
      "68250/68250 [==============================] - 30s 446us/sample - loss: 0.5355 - accuracy: 0.8476 - val_loss: 0.8826 - val_accuracy: 0.7730\n",
      "Epoch 39/50\n",
      "68250/68250 [==============================] - 30s 445us/sample - loss: 0.5335 - accuracy: 0.8478 - val_loss: 0.8842 - val_accuracy: 0.7719\n",
      "Epoch 40/50\n",
      "68250/68250 [==============================] - 30s 441us/sample - loss: 0.5316 - accuracy: 0.8479 - val_loss: 0.8856 - val_accuracy: 0.7744\n",
      "Epoch 41/50\n",
      "68250/68250 [==============================] - 30s 439us/sample - loss: 0.5302 - accuracy: 0.8488 - val_loss: 0.8859 - val_accuracy: 0.7730\n",
      "Epoch 42/50\n",
      "68250/68250 [==============================] - 30s 444us/sample - loss: 0.5294 - accuracy: 0.8489 - val_loss: 0.8854 - val_accuracy: 0.7725\n",
      "Epoch 43/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5267 - accuracy: 0.8494 - val_loss: 0.8893 - val_accuracy: 0.7724\n",
      "Epoch 44/50\n",
      "68250/68250 [==============================] - 30s 434us/sample - loss: 0.5247 - accuracy: 0.8504 - val_loss: 0.8896 - val_accuracy: 0.7720\n",
      "Epoch 45/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5240 - accuracy: 0.8503 - val_loss: 0.8897 - val_accuracy: 0.7740\n",
      "Epoch 46/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5220 - accuracy: 0.8511 - val_loss: 0.8914 - val_accuracy: 0.7751\n",
      "Epoch 47/50\n",
      "68250/68250 [==============================] - 30s 439us/sample - loss: 0.5204 - accuracy: 0.8516 - val_loss: 0.8953 - val_accuracy: 0.7747\n",
      "Epoch 48/50\n",
      "68250/68250 [==============================] - 30s 438us/sample - loss: 0.5191 - accuracy: 0.8514 - val_loss: 0.8934 - val_accuracy: 0.7738\n",
      "Epoch 49/50\n",
      "68250/68250 [==============================] - 30s 435us/sample - loss: 0.5179 - accuracy: 0.8516 - val_loss: 0.9001 - val_accuracy: 0.7746\n",
      "Epoch 50/50\n",
      "68250/68250 [==============================] - 30s 438us/sample - loss: 0.5169 - accuracy: 0.8532 - val_loss: 0.8977 - val_accuracy: 0.7744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4a54e8c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.astype('float64'), y_train.astype('float64'), batch_size=256,\n",
    "          epochs=50, validation_split=0.25,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6412250023302687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.81      7576\n",
      "         1.0       0.83      0.87      0.85      7250\n",
      "         2.0       0.93      0.93      0.93      6132\n",
      "         3.0       0.77      0.82      0.79      4233\n",
      "         4.0       0.73      0.70      0.72      3011\n",
      "         5.0       0.62      0.60      0.61      2379\n",
      "         6.0       0.78      0.78      0.78      1975\n",
      "         7.0       0.50      0.40      0.44      1673\n",
      "         8.0       0.54      0.49      0.51      1531\n",
      "         9.0       0.75      0.64      0.69      1151\n",
      "        10.0       0.64      0.62      0.63       963\n",
      "        11.0       0.52      0.32      0.40       612\n",
      "        12.0       0.54      0.42      0.47       346\n",
      "        13.0       0.38      0.32      0.35       168\n",
      "\n",
      "    accuracy                           0.77     39000\n",
      "   macro avg       0.66      0.62      0.64     39000\n",
      "weighted avg       0.76      0.77      0.77     39000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "val_pred = model.predict(X_test)\n",
    "y_pre = np.argmax(val_pred,axis=1)\n",
    "print(f1_score(y_test.to_numpy(), y_pre, average='macro'))\n",
    "print(classification_report(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_csv('train_set.csv', sep='\\t', nrows=130000)\n",
    "train_df['text'] = train_df['text'].apply(lambda x:x[0:2500])\n",
    "# train_df_sp_dim = train_df.text.str.split(' ',expand=True)\n",
    "sentence = list(train_df['text'].apply(lambda x:x.split(' ')[:500]))\n",
    "# train_df_sp_dim = pd.DataFrame(sentence).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2967</td>\n",
       "      <td>6758</td>\n",
       "      <td>339</td>\n",
       "      <td>2021</td>\n",
       "      <td>1854</td>\n",
       "      <td>3731</td>\n",
       "      <td>4109</td>\n",
       "      <td>3792</td>\n",
       "      <td>4149</td>\n",
       "      <td>1519</td>\n",
       "      <td>...</td>\n",
       "      <td>900</td>\n",
       "      <td>1635</td>\n",
       "      <td>3605</td>\n",
       "      <td>5028</td>\n",
       "      <td>3731</td>\n",
       "      <td>4109</td>\n",
       "      <td>3792</td>\n",
       "      <td>1866</td>\n",
       "      <td>3578</td>\n",
       "      <td>3915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4464</td>\n",
       "      <td>486</td>\n",
       "      <td>6352</td>\n",
       "      <td>5619</td>\n",
       "      <td>2465</td>\n",
       "      <td>4802</td>\n",
       "      <td>1452</td>\n",
       "      <td>3137</td>\n",
       "      <td>5778</td>\n",
       "      <td>5445</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7346</td>\n",
       "      <td>4068</td>\n",
       "      <td>5074</td>\n",
       "      <td>3747</td>\n",
       "      <td>5681</td>\n",
       "      <td>6093</td>\n",
       "      <td>1777</td>\n",
       "      <td>2226</td>\n",
       "      <td>7354</td>\n",
       "      <td>6301</td>\n",
       "      <td>...</td>\n",
       "      <td>3223</td>\n",
       "      <td>3750</td>\n",
       "      <td>5338</td>\n",
       "      <td>1952</td>\n",
       "      <td>6583</td>\n",
       "      <td>2230</td>\n",
       "      <td>1679</td>\n",
       "      <td>4811</td>\n",
       "      <td>6122</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7159</td>\n",
       "      <td>948</td>\n",
       "      <td>4866</td>\n",
       "      <td>2109</td>\n",
       "      <td>5520</td>\n",
       "      <td>2490</td>\n",
       "      <td>211</td>\n",
       "      <td>3956</td>\n",
       "      <td>5520</td>\n",
       "      <td>5492</td>\n",
       "      <td>...</td>\n",
       "      <td>3659</td>\n",
       "      <td>3370</td>\n",
       "      <td>3370</td>\n",
       "      <td>4646</td>\n",
       "      <td>1519</td>\n",
       "      <td>408</td>\n",
       "      <td>671</td>\n",
       "      <td>6560</td>\n",
       "      <td>1465</td>\n",
       "      <td>7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3646</td>\n",
       "      <td>3055</td>\n",
       "      <td>3055</td>\n",
       "      <td>2490</td>\n",
       "      <td>4659</td>\n",
       "      <td>6065</td>\n",
       "      <td>3370</td>\n",
       "      <td>5814</td>\n",
       "      <td>2465</td>\n",
       "      <td>5192</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9    ...   490  \\\n",
       "0  2967  6758   339  2021  1854  3731  4109  3792  4149  1519  ...   900   \n",
       "1  4464   486  6352  5619  2465  4802  1452  3137  5778  5445  ...     0   \n",
       "2  7346  4068  5074  3747  5681  6093  1777  2226  7354  6301  ...  3223   \n",
       "3  7159   948  4866  2109  5520  2490   211  3956  5520  5492  ...  3659   \n",
       "4  3646  3055  3055  2490  4659  6065  3370  5814  2465  5192  ...     0   \n",
       "\n",
       "    491   492   493   494   495   496   497   498   499  \n",
       "0  1635  3605  5028  3731  4109  3792  1866  3578  3915  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2  3750  5338  1952  6583  2230  1679  4811  6122  1903  \n",
       "3  3370  3370  4646  1519   408   671  6560  1465  7373  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_sp_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def trainWord2Vec(sentences):\n",
    "    sentences =  sentences # 读取分词后的 文本\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1,workers=4) # 训练模型\n",
    "\n",
    "    model.save('train_df_word2vec_100')\n",
    "trainWord2Vec(sentence)\n",
    "w2v_model = gensim.models.Word2Vec.load('train_df_word2vec_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "空间的词向量（100维）: [-2.3705518   3.2533045  -4.9708037  -0.79389894  1.9914211  -0.301904\n",
      " -5.6782956  -5.9995923  -2.359339   -4.846098   -3.6699686  -0.6368235\n",
      " -8.009765   -3.3419464  -1.7875142  -1.4862533   6.077497    1.8678385\n",
      "  1.8748858  -4.999699    0.26233327 -3.1009955   0.6131117  -2.3796966\n",
      " -1.6341016   0.3910347  -1.976291   -5.6062226  -0.3480148  -0.7850354\n",
      " -0.022438   -1.787291    0.39634222  1.8265374   1.0559077  -3.0334482\n",
      "  3.2604153   0.50285035 -1.8127171   0.20586187  4.0493565   4.5724907\n",
      " -1.6233882   0.08876958 -4.1339297  -4.347536    2.8274536   2.3509414\n",
      " -2.7328708   1.0841515   0.15885222 -4.98063     2.649037    1.5258813\n",
      " -2.2913294  -4.8186517  -1.1097127  -1.1256517  -2.3413734  -5.716834\n",
      "  0.03691056  6.4748163   0.67043227 -1.6508362   2.8216207  -1.0049047\n",
      "  0.9573532   2.1282399   1.7960088  -1.3148023  -4.5058     -0.6443786\n",
      " -1.4303478  -2.4840052  -4.3729525   1.9504524   0.14382927  0.31797022\n",
      "  2.1460848   4.357296   -1.4620496  -0.7882465   2.2368855  -2.1263554\n",
      "  0.8778546  -0.94875646  0.94224536 -1.484811   -4.7216835   1.7711579\n",
      " -0.36912197 -1.4303569   3.4799404   5.162195    0.30763036  0.45745692\n",
      " -4.428599   -3.042354    1.9796408   2.8311021 ]\n",
      "打印与空间最相近的5个词语： [('1432', 0.5287402272224426), ('2986', 0.4176838994026184), ('799', 0.4035533666610718), ('5180', 0.3978945016860962), ('933', 0.394009530544281)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\anaconda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "D:\\coding\\anaconda\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# 读取自己的词向量，并简单测试一下 效果。\n",
    "import gensim\n",
    "inp = 'train_df_word2vec_100'  # 读取词向量\n",
    "w2v_model = gensim.models.Word2Vec.load(inp)\n",
    "\n",
    "print('空间的词向量（100维）:',w2v_model['2967'])\n",
    "print('打印与空间最相近的5个词语：',w2v_model.most_similar('2967', topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 6308\n",
      "(6309, 100)\n"
     ]
    }
   ],
   "source": [
    "vocab_list = [(k, w2v_model.wv[k]) for k, v in w2v_model.wv.vocab.items()]\n",
    "word2idx = {\"_PAD\": 0} \n",
    "embeddings_matrix = np.zeros((len(w2v_model.wv.vocab.items()) + 1, w2v_model.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    word2idx[word] = i + 1\n",
    "    embeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "print('VOCAB_SIZE:',len(w2v_model.wv.index2word))\n",
    "print(embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_token_map =[]\n",
    "for rows in sentence:\n",
    "    rows_token = [word2idx[word] for word in rows]\n",
    "    list_token_map.append(rows_token)\n",
    "train_df_sp_dim = pd.DataFrame(list_token_map).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6309\n"
     ]
    }
   ],
   "source": [
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "def textRnn(max_sequence_length, max_token_num, embedding_dim, output_dim, model_img_path=None, embedding_matrix=None):\n",
    "    \"\"\" TextCNN: 1. embedding layers, 2.convolution layer, 3.max-pooling, 4.softmax layer. \"\"\"\n",
    "    x_input = Input(shape=(max_sequence_length,))\n",
    "    logging.info(\"x_input.shape: %s\" % str(x_input.shape))  # (?, 60)\n",
    "\n",
    "    if embedding_matrix is None:\n",
    "        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim,\n",
    "                          input_length=max_sequence_length)(x_input)\n",
    "    else:\n",
    "        x_emb = Embedding(input_dim=max_token_num, output_dim=embedding_dim, input_length=max_sequence_length,\n",
    "                          weights=[embedding_matrix], trainable=True)(x_input)\n",
    "    logging.info(\"x_emb.shape: %s\" % str(x_emb.shape))  # (?, 60, 300)\n",
    "\n",
    "    x_lstm1 = LSTM(128)(x_emb)\n",
    "    y = Dense(output_dim, activation='softmax')(x_lstm1)  # (?, 2)\n",
    "    logging.info(\"y.shape: %s \\n\" % str(y.shape))\n",
    "\n",
    "    model = Model([x_input], outputs=[y])\n",
    "    if model_img_path:\n",
    "        plot_model(model, to_file=model_img_path,\n",
    "                   show_shapes=True, show_layer_names=False)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='Adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df_sp_dim, train_df[['label']], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 100)          630900    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 749,954\n",
      "Trainable params: 749,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 6309\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS = 500\n",
    "CLASS_NUM = 14\n",
    "model = textRnn( max_sequence_length=MAX_WORDS,\n",
    "        max_token_num=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, output_dim=CLASS_NUM,embedding_matrix=embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68250 samples, validate on 22750 samples\n",
      "Epoch 1/50\n",
      "68250/68250 [==============================] - 40s 584us/sample - loss: 1.7712 - accuracy: 0.4336 - val_loss: 1.3960 - val_accuracy: 0.5583\n",
      "Epoch 2/50\n",
      "68250/68250 [==============================] - 31s 457us/sample - loss: 1.1042 - accuracy: 0.6587 - val_loss: 0.8821 - val_accuracy: 0.7337\n",
      "Epoch 3/50\n",
      "68250/68250 [==============================] - 31s 455us/sample - loss: 0.7888 - accuracy: 0.7595 - val_loss: 0.6828 - val_accuracy: 0.7928\n",
      "Epoch 4/50\n",
      "68250/68250 [==============================] - 31s 457us/sample - loss: 0.6131 - accuracy: 0.8175 - val_loss: 0.5759 - val_accuracy: 0.8265\n",
      "Epoch 5/50\n",
      "68250/68250 [==============================] - 31s 457us/sample - loss: 0.5161 - accuracy: 0.8451 - val_loss: 0.4947 - val_accuracy: 0.8506\n",
      "Epoch 6/50\n",
      "68250/68250 [==============================] - 31s 458us/sample - loss: 0.4419 - accuracy: 0.8691 - val_loss: 0.4468 - val_accuracy: 0.8692\n",
      "Epoch 7/50\n",
      "68250/68250 [==============================] - 31s 458us/sample - loss: 0.3847 - accuracy: 0.8849 - val_loss: 0.4557 - val_accuracy: 0.8637\n",
      "Epoch 8/50\n",
      "68250/68250 [==============================] - 31s 458us/sample - loss: 0.3659 - accuracy: 0.8908 - val_loss: 0.3631 - val_accuracy: 0.8899\n",
      "Epoch 9/50\n",
      "68250/68250 [==============================] - 31s 457us/sample - loss: 0.3297 - accuracy: 0.9011 - val_loss: 0.3570 - val_accuracy: 0.8919\n",
      "Epoch 10/50\n",
      "68250/68250 [==============================] - 31s 459us/sample - loss: 0.2901 - accuracy: 0.9124 - val_loss: 0.3288 - val_accuracy: 0.9022\n",
      "Epoch 11/50\n",
      "68250/68250 [==============================] - 31s 461us/sample - loss: 0.2751 - accuracy: 0.9171 - val_loss: 0.3275 - val_accuracy: 0.9029\n",
      "Epoch 12/50\n",
      "68250/68250 [==============================] - 31s 458us/sample - loss: 0.2573 - accuracy: 0.9217 - val_loss: 0.3167 - val_accuracy: 0.9047\n",
      "Epoch 13/50\n",
      "68250/68250 [==============================] - 31s 460us/sample - loss: 0.2273 - accuracy: 0.9314 - val_loss: 0.3007 - val_accuracy: 0.9099\n",
      "Epoch 14/50\n",
      "38656/68250 [===============>..............] - ETA: 12s - loss: 0.2009 - accuracy: 0.9393"
     ]
    }
   ],
   "source": [
    "model.fit(X_train.astype('float64'), y_train.astype('float64'), batch_size=256,\n",
    "          epochs=50, validation_split=0.25,use_multiprocessing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
